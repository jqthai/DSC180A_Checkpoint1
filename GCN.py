# -*- coding: utf-8 -*-
"""ParticipationAssignment2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hnm3VgqWmBmG_MGq24AMxAJGid6r8fzD
"""

!pip install torch-geometric

from torch_geometric.datasets import Planetoid
import torch

dataset = Planetoid(root='/tmp/Cora', name='Cora')

import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv

#node classification

class GCN(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = GCNConv(dataset.num_node_features, 16)
        self.conv2 = GCNConv(16, dataset.num_classes)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index

        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.conv2(x, edge_index)

        return F.log_softmax(x, dim=1)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = GCN().to(device)
data = dataset[0].to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)

model.train()
for epoch in range(200):
    optimizer.zero_grad()
    out = model(data)
    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])
    loss.backward()
    optimizer.step()

model.eval()
pred = model(data).argmax(dim=1)
correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()
acc = int(correct) / int(data.test_mask.sum())
print(f'Accuracy: {acc:.4f}')

def layers(n):
  accuracies = []

  class GCN(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = GCNConv(dataset.num_node_features, 16)
        self.conv2 = GCNConv(16, dataset.num_classes)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index

        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.conv2(x, edge_index)

        return F.log_softmax(x, dim=1)

  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
  model = GCN().to(device)
  data = dataset[0].to(device)
  optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)

  model.train()
  for epoch in range(200):
    optimizer.zero_grad()
    out = model(data)
    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])
    loss.backward()
    optimizer.step()

  model.eval()
  pred = model(data).argmax(dim=1)
  correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()
  acc = int(correct) / int(data.test_mask.sum())

  accuracies.append(acc)

  return accuracies

#visualization
import networkx as nx
import matplotlib.pyplot as plt
from torch_geometric.utils import to_networkx

#transform pytorch cora graph into networkx to visualize it
cora_visualization = to_networkx(data, to_undirected=True)

plt.figure(figsize=(7, 7))
node_colors = pred.numpy()
colormap = plt.get_cmap('seismic')
pos = nx.spring_layout(cora_visualization, seed=42)
nx.draw_networkx_nodes(cora_visualization, pos, node_size=300, node_color=node_colors, cmap=colormap, alpha=0.8)
nx.draw_networkx_edges(cora_visualization, pos, alpha=0.5)

predicted_labels = labels = {i: str(pred.numpy()[i].item()) for i in range(data.num_nodes)}
nx.draw_networkx_labels(cora_visualization, pos, predicted_labels, font_size=8)

#graph classification for IMDB dataset
from torch_geometric.datasets import TUDataset
imdb_dataset = TUDataset(root='/tmp/IMDB-BINARY', name='IMDB-BINARY')

imdb_dataset = imdb_dataset.shuffle()

training_imdb_dataset = imdb_dataset[:int(len(imdb_dataset) * 0.8)]
test_imdb_dataset = imdb_dataset[int(len(imdb_dataset) * 0.8):]

from torch_geometric.loader import DataLoader

imdb_train_loader = DataLoader(training_imdb_dataset, batch_size=64, shuffle=True)
imdb_test_loader = DataLoader(test_imdb_dataset, batch_size=64, shuffle=False)

for step, data in enumerate(imdb_train_loader):
    print(f'Step {step + 1}:')
    print('=======')
    print(f'Number of graphs in the current batch: {data.num_graphs}')
    print(data)
    print()

from torch.nn import Linear
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
from torch_geometric.nn import global_mean_pool


class GCN(torch.nn.Module):
    def __init__(self, hidden_channels):
        super(GCN, self).__init__()
        torch.manual_seed(12345)
        self.conv1 = GCNConv(imdb_dataset.num_node_features, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        self.conv3 = GCNConv(hidden_channels, hidden_channels)
        self.lin = Linear(hidden_channels, imdb_dataset.num_classes)

    def forward(self, x, edge_index, batch):
        if x is None:
            x = torch.ones((edge_index.max().item() + 1, 1), device=edge_index.device)
        # 1. Obtain node embeddings
        x = self.conv1(x, edge_index)
        x = x.relu()
        x = self.conv2(x, edge_index)
        x = x.relu()
        x = self.conv3(x, edge_index)

        # 2. Readout layer
        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]

        # 3. Apply a final classifier
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.lin(x)

        return x

model = GCN(hidden_channels=64)
print(model)

from IPython.display import Javascript
display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))

#model = GCN(hidden_channels=64)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = torch.nn.CrossEntropyLoss()

def train():
    model.train()

    for data in imdb_train_loader:  # Iterate in batches over the training dataset.
         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.
         loss = criterion(out, data.y)  # Compute the loss.
         loss.backward()  # Derive gradients.
         optimizer.step()  # Update parameters based on gradients.
         optimizer.zero_grad()  # Clear gradients.

def test(loader):
     model.eval()

     correct = 0
     for data in loader:  # Iterate in batches over the training/test dataset.
         out = model(data.x, data.edge_index, data.batch)
         pred = out.argmax(dim=1)  # Use the class with highest probability.
         correct += int((pred == data.y).sum())  # Check against ground-truth labels.
     return correct / len(loader.dataset)  # Derive ratio of correct predictions.


for epoch in range(1, 171):
    train()
    train_acc = test(imdb_train_loader)
    test_acc = test(imdb_test_loader)
    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')

enzyme_dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')
len(enzyme_dataset)

#graph classification for ENZYME dataset

enzyme_data = enzyme_dataset[0]

enzyme_dataset = enzyme_dataset.shuffle()

train_enzyme_dataset = enzyme_dataset[:540]
test_enzyme_dataset = enzyme_dataset[540:]

print('Train dataset:', train_enzyme_dataset)
print('Test dataset:', test_enzyme_dataset)

from torch_geometric.loader import DataLoader

enzyme_train_loader = DataLoader(train_enzyme_dataset, batch_size=64, shuffle=True)
enzyme_test_loader = DataLoader(test_enzyme_dataset, batch_size=64, shuffle=False)

for step, data in enumerate(enzyme_train_loader):
    print(f'Step {step + 1}:')
    print('=======')
    print(f'Number of graphs in the current batch: {data.num_graphs}')

from torch.nn import Linear
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
from torch_geometric.nn import global_mean_pool


class GCN(torch.nn.Module):
    def __init__(self, hidden_channels):
        super(GCN, self).__init__()
        torch.manual_seed(12345)
        self.conv1 = GCNConv(enzyme_dataset.num_node_features, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        self.conv3 = GCNConv(hidden_channels, hidden_channels)
        self.lin = Linear(hidden_channels, enzyme_dataset.num_classes)

    def forward(self, x, edge_index, batch):
        if x is None:
            x = torch.ones((edge_index.max().item() + 1, 1), device=edge_index.device)

        # 1. Obtain node embeddings
        x = self.conv1(x, edge_index)
        x = x.relu()
        x = self.conv2(x, edge_index)
        x = x.relu()
        x = self.conv3(x, edge_index)

        # 2. Readout layer
        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]

        # 3. Apply a final classifier
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.lin(x)

        return x

model = GCN(hidden_channels=64)
print(model)

from IPython.display import Javascript
display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))

model = GCN(hidden_channels=64)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = torch.nn.CrossEntropyLoss()

def train():
    model.train()

    for data in enzyme_train_loader:  # Iterate in batches over the training dataset.
         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.
         loss = criterion(out, data.y)  # Compute the loss.
         loss.backward()  # Derive gradients.
         optimizer.step()  # Update parameters based on gradients.
         optimizer.zero_grad()  # Clear gradients.

def test(loader):
     model.eval()

     correct = 0
     for data in loader:  # Iterate in batches over the training/test dataset.
         out = model(data.x, data.edge_index, data.batch)
         pred = out.argmax(dim=1)  # Use the class with highest probability.
         correct += int((pred == data.y).sum())  # Check against ground-truth labels.
     return correct / len(loader.dataset)  # Derive ratio of correct predictions.


for epoch in range(1, 171):
    train()
    train_acc = test(enzyme_train_loader)
    test_acc = test(enzyme_test_loader)
    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')

